<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Natural Zoom Gesture on Frugal Devices</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheet/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheet/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Zoom Gesture</h1>
      <h2 class="project-tagline">Natural Zoom Gesture Interaction on Frugal Devices for AR Application</h2>
      <a href="https://youtu.be/at8uBJiehQ8" class="btn">zOOm Demo</a>
     
    </section>

    <section class="main-content">
      <h3>
<a id="welcome-to-ZoomGesture" class="anchor" href="#welcome-to-telepresence-roi" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Natural zOOm Gesture</h3>

<p>In air-gesture based Natural interaction has important role in human-computer interface. It Provides significant part in non verbal communication, hand gestures are playing vital role in our dailylife. Gesture Recognition has a wide area of application including human machine interaction, sign language, immersive game technology etc. However, many applications have been proposed to incorporate with additional wearable sensors and some specific training for each user may be required. These limitations make it inconvenience for using in the real world. This paper presents novel one-handed gesture based interaction for zoom in/out manipulation of FOV, which is fast and easy to implement without using any additional. The manipulation of zoom command with natural gesture is to pinch or to stretch fingers, like on touch-devices. The goal of this research is to extending the interaction space around mobile devices for augmented reality on Cardboard, i.e. applications where users look at the live image of the devices video camera.The technique uses only the RGB camera now commonplace on off-the-shelf mo- bile devices. Our algorithm robustly recognizes Zoom in/out in-air gestures, supporting user variation, and varying lighting conditions. We demonstrate that our algorithm runs in real-time on unmodified mobile devices, including resource-constrained Smartphones.</p>

<h3>
<a id="the-idea" class="anchor" href="#the-idea" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The Idea</h3>

<p><img src="https://github.com/JitenderMaurya/ZoomGesture/blob/master/img/Zoomflow.png?raw=true?" alt="Proposed Method "></p>

<p>We  present an  approach  for  marker-less  and  real-time touch-less gestures for Zoom in/out on wearables in FPV by tracking
  the movement of Thumb and Index finger. Our  approach  is  particularly  suitable  as  most  of  the smartphones available in 
  the market are not equipped with built-in depth sensor posing additional challenges. The main blocks of the algorithm are: 
  (i) hand segmentaion, (ii) stable hand detection, (iii) left/right hand detection (iv) thumb and index finger segmentation 
  (v) tracking & analysing thumb & index finger movement. First we average out similar texture region and highlight edges between different texture. Skin pixel detection followed by largest contour segmentation gives the hand region in the user's FOV. Left/Right hand detection is done by determing the slope of hand, because hand orientation will be of fix type in Google Cardboard/HMD.
  If Left hand is detected, right most point on hand countour is extreme point. Similarly, if right hand is detected, left most point on 
  hand countour is extreme point. After the left/right hand is detected, center of extreme point & centroid of contour is calculated.
  The hand region towards centroid of contour from center is removed. Now of the largest two blob one is thumb & other is index
  finger. The distance between fingres is analysed to classify Zoom gesture. We conducted experiments which  demonstrates  that our method of Zoom 
  Gesture, work in real time, on device in complex background. </p>

<h3>
<a id="the-idea" class="anchor" href="#the-idea" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Demo Video</h3>
  <iframe width="560" height="315" src="https://www.youtube.com/embed/at8uBJiehQ8" frameborder="0" allowfullscreen></iframe>
      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/ondevicezoomGesture/zOOmgesture">Zoom Gesture</a> is maintained by <a href="https://github.com/ondevicezoomGesture">jitm009</a>.</span>

      </footer>

    </section>

  </body>
</html>
